pipeline {
    agent any

    environment {
          SBT_HOME = tool name: 'sbt-1.5.4', type: 'org.jvnet.hudson.plugins.SbtPluginBuilder$SbtInstallation'
          PATH = "${env.SBT_HOME}/bin:${env.PATH}"
    }

    options {
        disableConcurrentBuilds()
    }

    parameters {
        string(name: 'BRANCH', defaultValue: params.BRANCH ?:'', description: 'Enter name of the branch to be polled')
        string(name: 'JENKINS_JARS_BUCKET', defaultValue: params.JENKINS_JARS_BUCKET ?: '', description: 'Enter S3 bucket name where jars will be stored')
    }

    stages {
        stage('Build') {
            steps {
                sh 'sbt clean assembly'
            }
        }
        stage('Test') {
            steps {
                echo 'Testing..'
            }
        }
        stage('Store') {
            steps {
                 withAWS(region: 'us-east-2',credentials: 'sigma-aws-creds') {
                    script{
                        def sourceCrawlerJar = "./jars/crawler.jar"
                        def sourceSparkJar = "./jars/spark.jar"

                        def newCrawlerJar = "./jars/${BRANCH}/${BUILD_ID}/crawler.jar"
                        def newSparkJar = "./jars/${BRANCH}/${BUILD_ID}/spark.jar"

                        writeFile(file: newCrawlerJar, encoding: "UTF-8", text: readFile(file: sourceCrawlerJar, encoding: "UTF-8"))
                        writeFile(file: newSparkJar, encoding: "UTF-8", text: readFile(file: sourceSparkJar, encoding: "UTF-8"))
                    }
                    s3Upload(bucket: "${JENKINS_JARS_BUCKET}", includePathPattern: "jars/*/*/*.jar")
                 }
            }
        }
    }
    post {
      always {
        cleanWs()
        dir("${env.WORKSPACE}@tmp") {
          deleteDir()
        }
        dir("${env.WORKSPACE}@script") {
          deleteDir()
        }
        dir("${env.WORKSPACE}@script@tmp") {
          deleteDir()
        }
      }
    }
}
